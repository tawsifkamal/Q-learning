{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO16a8Qo1pM/d2rbHPv9+i/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawsifkamal/Q-learning/blob/main/FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZm2JJhaWrYs"
      },
      "source": [
        "# **Reinforcement Learning in FrozenLake**\r\n",
        "#### In this notebook, we will use a basic Reinforcement learning Q-learning algorithm to train an agent to play open AI Gym's FrozenLake game.\r\n",
        "\r\n",
        "**The steps to implement the algorithm are as follows:**\r\n",
        "\r\n",
        "1. Initialize the FrozenLake environment, define the global variables and hyperparameters, and create an empty Q-table.\r\n",
        "\r\n",
        "2. Iterate thorough 10000 episodes and 100 time steps in order to train the RL agent by updating the Q-values in the Q-table through the Bellman's Optimality Equation.\r\n",
        "\r\n",
        "3. Play the game by selecting the optimal actions from the updated Q-table and watch the agent win! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFWeAkrAANr3"
      },
      "source": [
        "### **1. Setting up the Environment, global variables, and Q-table**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh11dZ0-cxTJ"
      },
      "source": [
        "# import the modules needed\r\n",
        "import numpy as np\r\n",
        "import gym\r\n",
        "import random \r\n",
        "import time \r\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHz7jq2Qc6vj"
      },
      "source": [
        "# initializing the environment and making the q_table\r\n",
        "env = gym.make('FrozenLake-v0')\r\n",
        "action_space = env.action_space.n\r\n",
        "state_space = env.observation_space.n\r\n",
        "q_table = np.zeros((state_space, action_space))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHRABuKEc-rH",
        "outputId": "d3066b7c-17a0-4987-b56e-5feea8b0c197"
      },
      "source": [
        "#Check for shape\r\n",
        "q_table.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22-M39A2djA-"
      },
      "source": [
        "# Setting the hyperparameters \r\n",
        "num_episodes = 10000\r\n",
        "rewards_all_episodes = []\r\n",
        "exploration_rate = 1\r\n",
        "exploration_decay_rate = 0.001\r\n",
        "max_exploration_rate = 1\r\n",
        "min_exploration_rate = 0.01\r\n",
        "lr = 0.1\r\n",
        "discount_rate = 0.99 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIuah1_tAjZf"
      },
      "source": [
        "### **2. Training the RL agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV8kI5x8eALV"
      },
      "source": [
        "for episode in range(num_episodes):\r\n",
        "  state = env.reset()\r\n",
        "  done = False \r\n",
        "\r\n",
        "  reward_current_episode = 0\r\n",
        "\r\n",
        "  for step in range(100):\r\n",
        "\r\n",
        "    # Exploration-exploitation trade-off\r\n",
        "    threshold = random.uniform(0, 1)\r\n",
        "    if threshold > exploration_rate:\r\n",
        "      action = np.argmax(q_table[state, :])\r\n",
        "    else:\r\n",
        "      action = env.action_space.sample()\r\n",
        "    \r\n",
        "    #taking the action\r\n",
        "    new_state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "    # Updating the q_values \r\n",
        "    q_table[state, action] = (1 - lr) * q_table[state, action] + lr * (reward + discount_rate * np.max(q_table[new_state, :]))\r\n",
        "\r\n",
        "    state = new_state \r\n",
        "    reward_current_episode += reward\r\n",
        "\r\n",
        "    if done: \r\n",
        "      break\r\n",
        "    \r\n",
        "  # Eploration decay formula\r\n",
        "  exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\r\n",
        "\r\n",
        "  rewards_all_episodes.append(reward_current_episode)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDIPlpN3gkOh",
        "outputId": "a6774bae-ad5f-452a-8f77-cc064caae8f5"
      },
      "source": [
        "# Displaying the metrics\r\n",
        "reward_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\r\n",
        "count = 1000\r\n",
        "print('\\n ***** Average number of rewards per one thousnad episodes ***** \\n')\r\n",
        "for reward in reward_per_thousand_episodes:\r\n",
        "  print(count, ': ', str(sum(reward)/1000))\r\n",
        "  count += 1000\r\n",
        "\r\n",
        "print(' \\n ****** Updated Q-table ******')\r\n",
        "print(q_table)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ***** Average number of rewards per one thousnad episodes ***** \n",
            "\n",
            "1000 :  0.026\n",
            "2000 :  0.121\n",
            "3000 :  0.383\n",
            "4000 :  0.559\n",
            "5000 :  0.638\n",
            "6000 :  0.65\n",
            "7000 :  0.665\n",
            "8000 :  0.7\n",
            "9000 :  0.675\n",
            "10000 :  0.679\n",
            " \n",
            " ****** Updated Q-table ******\n",
            "[[0.54268927 0.48679449 0.49175608 0.48594515]\n",
            " [0.23278903 0.23265411 0.18026242 0.4385909 ]\n",
            " [0.35750983 0.24838672 0.18525946 0.22710666]\n",
            " [0.18628561 0.01033244 0.00754911 0.01253793]\n",
            " [0.55792456 0.39664852 0.33856906 0.36500834]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.21612097 0.14013744 0.31725624 0.08513492]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.40508375 0.29192378 0.40869606 0.5908078 ]\n",
            " [0.40037599 0.63823276 0.46624761 0.36434178]\n",
            " [0.63322769 0.45112699 0.33991055 0.27445116]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.41654935 0.47333203 0.75989377 0.57138342]\n",
            " [0.73903394 0.84561353 0.74001725 0.76299689]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jrwYS12AxfR"
      },
      "source": [
        "### **3. Agent playing the game**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbtjIMjf6eWx",
        "outputId": "7d544799-77ec-4bfb-fced-816fc56e24ae"
      },
      "source": [
        "# Playing the game \r\n",
        "for episode in range(3): \r\n",
        "  state = env.reset()\r\n",
        "\r\n",
        "  for step in range(100): \r\n",
        "    clear_output(wait=True)\r\n",
        "    time.sleep(1)\r\n",
        "    action = np.argmax(q_table[state, :])\r\n",
        "    new_state, reward, done, info = env.step(action)\r\n",
        "    state = new_state\r\n",
        "    env.render()\r\n",
        "\r\n",
        "    if reward == 1: \r\n",
        "      print('*****You won the game!*****')\r\n",
        "      break\r\n",
        "\r\n",
        "    elif done == True:\r\n",
        "      print('*****You lost oh no!*****')\r\n",
        "      time.sleep(1)\r\n",
        "      break "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "*****You won the game!*****\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}